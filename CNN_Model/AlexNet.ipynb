{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb711ee6",
   "metadata": {},
   "source": [
    "# AlexNet fine-tuning & SVM classification\n",
    "\n",
    "In this assignment, we investigate the use of transfer learning for image classification by leveraging a pre-trained convolutional neural network, specifically AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce3935d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data_dir: Path, batch_size: int) -> tuple:\n",
    "    \"\"\"Create PyTorch data loaders for train, validation and test sets.\"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # TRANSFORMS\n",
    "    # -----------------------------\n",
    "\n",
    "    # Define preprocessing + augmentation for training images.\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),                # Resize all images to 224×224\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert 1-channel grayscale -> 3-channel (AlexNet expects RGB)\n",
    "        transforms.ToTensor(),                        # Convert PIL image -> PyTorch tensor (range [0,1])\n",
    "        transforms.Normalize(                         # Normalize using ImageNet mean/std\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Define preprocessing for validation/test (no augmentation).\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),                # Resize same as training\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert grayscale -> 3 channels\n",
    "        transforms.ToTensor(),                        # Convert to tensor\n",
    "        transforms.Normalize(                         # Normalize with same stats\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # -----------------------------\n",
    "    # DATASETS\n",
    "    # -----------------------------\n",
    "\n",
    "    # Load images from train folder and apply training transforms\n",
    "    train_dataset = datasets.ImageFolder(str(data_dir / 'train'), transform=transform_train)\n",
    "\n",
    "    # Load validation dataset with test transforms (no augmentation)\n",
    "    val_dataset = datasets.ImageFolder(str(data_dir / 'val'), transform=transform_test)\n",
    "\n",
    "    # Load test dataset\n",
    "    test_dataset = datasets.ImageFolder(str(data_dir / 'test'), transform=transform_test)\n",
    "\n",
    "    # -----------------------------\n",
    "    # DATALOADERS\n",
    "    # -----------------------------\n",
    "\n",
    "    # Create dataloader for training\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,          # Shuffle training batches\n",
    "        num_workers=4          # Number of background workers loading data\n",
    "    )\n",
    "\n",
    "    # Validation dataloader (no shuffle)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Test dataloader (no shuffle)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Return loaders as a tuple\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c950ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_last_layer(data_dir: Path, batch_size: int = 32, epochs: int = 10, learning_rate: float = 0.001) -> None:\n",
    "    \"\"\"Fine-tune only the final fully connected layer of a pre-trained AlexNet.\"\"\"\n",
    "    \n",
    "    # Select GPU if available, otherwise CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create train/validation/test dataloaders\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(data_dir, batch_size)\n",
    "\n",
    "    # Load AlexNet pre-trained on ImageNet\n",
    "    model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "    model.to(device)  # Move model to chosen device\n",
    "\n",
    "    # Freeze all layers so they will NOT update during training\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Extract number of input features to the final linear layer\n",
    "    num_features = model.classifier[6].in_features\n",
    "\n",
    "    # Replace the last classification layer with a new one for 15 classes\n",
    "    model.classifier[6] = nn.Linear(num_features, 15)\n",
    "    model.classifier[6].to(device)\n",
    "\n",
    "    # Only train the parameters of the new final layer\n",
    "    parameters_to_update = model.classifier[6].parameters()\n",
    "\n",
    "    # Loss function for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # SGD optimizer with momentum, applied ONLY to the last layer\n",
    "    optimizer = optim.SGD(parameters_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    best_val_acc = 0.0  # Track best validation accuracy\n",
    "\n",
    "    # -------------------------\n",
    "    # TRAINING LOOP\n",
    "    # -------------------------\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0  # Track total loss\n",
    "        correct = 0         # Track correct predictions\n",
    "        total = 0           # Track total samples\n",
    "\n",
    "        # Iterate over training batches\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "            optimizer.zero_grad()       # Reset gradients\n",
    "            outputs = model(inputs)     # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()             # Backpropagation\n",
    "            optimizer.step()            # Update weights (only last layer)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)  # Accumulate batch loss\n",
    "            _, preds = outputs.max(1)                     # Predicted class indices\n",
    "            correct += preds.eq(labels).sum().item()      # Count correct predictions\n",
    "            total += labels.size(0)                       # Add batch size\n",
    "\n",
    "        # Compute epoch-level training stats\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # -------------------------\n",
    "        # VALIDATION LOOP\n",
    "        # -------------------------\n",
    "        model.eval()   # Switch to eval mode\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculations during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = outputs.max(1)\n",
    "                val_correct += preds.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs} - \"\n",
    "            f\"train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, val_acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improved\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'finetuned_alexnet_best.pth')\n",
    "\n",
    "    print(f\"Best validation accuracy: {best_val_acc * 100:.2f}%\")\n",
    "\n",
    "    # -------------------------\n",
    "    # TESTING LOOP\n",
    "    # -------------------------\n",
    "\n",
    "    # Load the best-performing checkpoint\n",
    "    model.load_state_dict(torch.load('finetuned_alexnet_best.pth', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "            test_correct += preds.eq(labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    # Compute final test accuracy\n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"Test accuracy (fine-tuning last layer): {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b47335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from itertools import combinations\n",
    "\n",
    "class DAGSVM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Multiclass SVM using a Directed Acyclic Graph over One-vs-One binary classifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_estimator=None):\n",
    "        # base_estimator: an unfitted sklearn SVM \n",
    "        self.base_estimator = base_estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)      # Convert X to NumPy array\n",
    "        y = np.asarray(y)      # Convert y to NumPy array\n",
    "\n",
    "        self.classes_ = np.unique(y)   # All unique class labels\n",
    "        self.pair_clfs_ = {}           # Dict to store all pairwise classifiers\n",
    "\n",
    "        # Train one binary classifier for each pair of classes\n",
    "        for ci, cj in combinations(self.classes_, 2):  # Generate all (ci, cj) pairs\n",
    "            mask = np.logical_or(y == ci, y == cj)     # Keep only samples of class ci or cj\n",
    "            X_ij = X[mask]                             # Filter features\n",
    "            y_ij = y[mask]                             # Filter labels\n",
    "\n",
    "            if self.base_estimator is None:\n",
    "                # Default classifier: LinearSVC\n",
    "                clf = LinearSVC(C=1.0, dual=False, max_iter=10000)\n",
    "            else:\n",
    "                clf = clone(self.base_estimator)       # Clone user-provided estimator\n",
    "\n",
    "            clf.fit(X_ij, y_ij)                        # Train on the two-class subset\n",
    "            self.pair_clfs_[(ci, cj)] = clf            # Store classifier for this pair\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        # DAG prediction: start with all classes and eliminate losers\n",
    "        remaining = list(self.classes_)           # The set of candidate classes\n",
    "        i = 0                                     # Pointer to first class\n",
    "        j = len(remaining) - 1                    # Pointer to last class\n",
    "\n",
    "        # Continue eliminating until only one class remains\n",
    "        while len(remaining) > 1:\n",
    "            ci, cj = remaining[i], remaining[j]  # Compare the \"left\" and \"right\" classes\n",
    "\n",
    "            # Retrieve classifier for (ci, cj) regardless of order\n",
    "            key = (ci, cj) if (ci, cj) in self.pair_clfs_ else (cj, ci)\n",
    "            clf = self.pair_clfs_[key]\n",
    "\n",
    "            pred = clf.predict(x.reshape(1, -1))[0]  # Predict which of ci/cj wins\n",
    "\n",
    "            # Winner stays in DAG, loser gets removed\n",
    "            if pred == ci:\n",
    "                # ci wins -> remove cj\n",
    "                remaining.pop(j)\n",
    "                j = len(remaining) - 1               # Update end pointer\n",
    "            else:\n",
    "                # cj wins -> remove ci\n",
    "                remaining.pop(i)\n",
    "                j = len(remaining) - 1               # Update end pointer\n",
    "\n",
    "        return remaining[0]  # Only one class left -> final prediction\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)                          # Ensure NumPy array\n",
    "        # Predict sample-by-sample using DAG logic\n",
    "        return np.array([self._predict_single(x) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95519145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_feature_extraction(data_dir: Path, batch_size: int = 32) -> None:\n",
    "    \"\"\"Use AlexNet as a feature extractor and train a multi-class linear SVM.\"\"\"\n",
    "\n",
    "    # Select GPU if available, otherwise CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # TRANSFORMS & DATASETS — same as fine-tuning but NO augmentation\n",
    "    # ----------------------------------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),               # Resize input for AlexNet\n",
    "        transforms.Grayscale(num_output_channels=3), # Convert grayscale -> RGB-like 3 channels\n",
    "        transforms.ToTensor(),                       # Convert to tensor\n",
    "        transforms.Normalize(                        # Normalise using ImageNet statistics\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # ImageFolder automatically assigns class labels based on directory names\n",
    "    train_dataset = datasets.ImageFolder(str(data_dir / 'train'), transform=transform)\n",
    "    val_dataset   = datasets.ImageFolder(str(data_dir / 'val'),   transform=transform)\n",
    "    test_dataset  = datasets.ImageFolder(str(data_dir / 'test'),  transform=transform)\n",
    "\n",
    "    # Data loaders: shuffle=False because order matters for feature extraction\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # LOAD ALEXNET AND BUILD FEATURE EXTRACTOR (remove classifier)\n",
    "    # ----------------------------------------------------\n",
    "    alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)  # Pretrained on ImageNet\n",
    "    alexnet.to(device)\n",
    "    alexnet.eval()  # Disable dropout, batch norm updates\n",
    "\n",
    "    # Create sequential model: convolutional layers + average pooling + flatten\n",
    "    feature_extractor = nn.Sequential(\n",
    "        alexnet.features,   # Convolutional backbone\n",
    "        alexnet.avgpool,    # Adaptive average pooling\n",
    "        nn.Flatten(),       # Flatten output into a 1D vector\n",
    "    ).to(device)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # FEATURE EXTRACTION FUNCTION\n",
    "    # ----------------------------------------------------\n",
    "    def extract_features(loader: DataLoader) -> tuple:\n",
    "        features_list = []  # store extracted features\n",
    "        labels_list = []    # store labels matching features\n",
    "\n",
    "        with torch.no_grad():  # no gradient computation\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                feats = feature_extractor(inputs)    # forward pass through CNN backbone\n",
    "                features_list.append(feats.cpu().numpy())  # save features to CPU array\n",
    "                labels_list.append(labels.numpy())         # save labels\n",
    "\n",
    "        # Concatenate batches into full dataset arrays\n",
    "        features = np.concatenate(features_list, axis=0)\n",
    "        labels   = np.concatenate(labels_list,   axis=0)\n",
    "        return features, labels\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # EXTRACT FEATURES FOR TRAIN + VAL (training set)\n",
    "    # ----------------------------------------------------\n",
    "    print(\"Extracting features for training and validation...\")\n",
    "    train_features, train_labels = extract_features(train_loader)\n",
    "    val_features,   val_labels   = extract_features(val_loader)\n",
    "\n",
    "    # Merge training and validation sets for SVM training\n",
    "    X_train = np.vstack((train_features, val_features))\n",
    "    y_train = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # EXTRACT FEATURES FOR TEST SET\n",
    "    # ----------------------------------------------------\n",
    "    print(\"Extracting features for test set...\")\n",
    "    X_test, y_test = extract_features(test_loader)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # SCALE FEATURES (VERY IMPORTANT FOR SVMs)\n",
    "    # ----------------------------------------------------\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)   # Fit on train, transform train\n",
    "    X_test_scaled  = scaler.transform(X_test)        # Transform test only\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # TRAIN DAG-SVM (ONE-VS-ONE CLASSIFIERS + DAG INFERENCE)\n",
    "    # ----------------------------------------------------\n",
    "    print(\"Training multi-class DAGSVM\")\n",
    "\n",
    "    base_svm = LinearSVC(C=1.0, dual=False, max_iter=10)  # Base binary classifier\n",
    "    svm = DAGSVM(base_estimator=base_svm)                  # Wrap into DAG-SVM structure\n",
    "    svm.fit(X_train_scaled, y_train)                       # Train all pairwise SVMs\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # EVALUATE ON TEST SET\n",
    "    # ----------------------------------------------------\n",
    "    y_pred = svm.predict(X_test_scaled)          # Predict using DAG traversal\n",
    "    acc = accuracy_score(y_test, y_pred)         # Compute test accuracy\n",
    "\n",
    "    print(f\"Test accuracy (SVM on features): {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4d27eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/leonardoangellotti/Desktop/universita/Comp_Vision/CNN/data_augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e4c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/10 - train_loss: 0.7389, train_acc: 0.7559, val_acc: 0.8933\n",
      "Epoch 2/10 - train_loss: 0.3577, train_acc: 0.8837, val_acc: 0.9022\n",
      "Epoch 3/10 - train_loss: 0.2708, train_acc: 0.9141, val_acc: 0.8889\n",
      "Epoch 4/10 - train_loss: 0.2394, train_acc: 0.9233, val_acc: 0.9289\n",
      "Epoch 5/10 - train_loss: 0.2080, train_acc: 0.9320, val_acc: 0.9022\n",
      "Epoch 6/10 - train_loss: 0.1913, train_acc: 0.9378, val_acc: 0.9156\n",
      "Epoch 7/10 - train_loss: 0.1723, train_acc: 0.9457, val_acc: 0.9200\n",
      "Epoch 8/10 - train_loss: 0.1636, train_acc: 0.9490, val_acc: 0.9022\n",
      "Epoch 9/10 - train_loss: 0.1532, train_acc: 0.9553, val_acc: 0.8978\n",
      "Epoch 10/10 - train_loss: 0.1450, train_acc: 0.9533, val_acc: 0.8978\n",
      "Best validation accuracy: 92.89%\n",
      "Test accuracy (fine-tuning last layer): 87.04%\n"
     ]
    }
   ],
   "source": [
    "fine_tune_last_layer(data_dir) # 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c772083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Extracting features for training and validation...\n",
      "Extracting features for test set...\n",
      "Training multi-class DAGSVM (one-vs-one + DAG inference)... this may take a few minutes\n",
      "Test accuracy (SVM on features): 85.56%\n"
     ]
    }
   ],
   "source": [
    "svm_feature_extraction(data_dir) # 3 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
